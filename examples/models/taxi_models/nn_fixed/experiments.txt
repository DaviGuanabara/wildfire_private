Experiment 1:

Starting the learning rate at 0.001 and increasing to 0.01 during 10 epochs

after 27 epochs stopped training and started again with learning rate of 0.005:

[INFO] epoch: 13/2500, training loss: 62.56579338420519, validation loss: 62.469215774536146
[INFO] lr: 0.01
/Users/leonardo/Documents/WildFire/Paper-STML-new/paper-stml-2023/filipe/taxi/nn_fixed/training.py:16: MatplotlibDeprecationWarning: The resize_event function was deprecated in Matplotlib 3.6 and will be removed two minor releases later. Use callbacks.process('resize_event', ResizeEvent(...)) instead.
  plt.figure()
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 62.469215774536146

...

[INFO] epoch: 7/2500, training loss: 61.45617133920844, validation loss: 62.35835247039795
[INFO] lr: 0.005
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 62.35835247039795

Evaluating on test set:
Mean squared error: 10655.86058789566
RMSE: 103.22722793863865

Experiment 2:

Training with learning rate constant of 0.001

[INFO] lr: 0.001
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 61.954117202758795
[INFO] epoch: 15/2500, training loss: 63.219124273820356, validation loss: 62.314569854736334

Experiment 3:

Training with learning rate constant of 0.005 (0.01 seemed to high, and 0.001 too low)

[INFO] epoch: 11/2500, training loss: 62.741713740608915, validation loss: 62.195136833190915
[INFO] lr: 0.005
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 62.195136833190915


Experiment 4:

Latitude and longitude were not normalized in the previous experiments (were between 0 and 49)

Added the following lines:

train_data[:,1] = train_data[:,1] / 49.0
train_data[:,2] = train_data[:,2] / 49.0

...

val_data[:,1] = val_data[:,1] / 49.0
val_data[:,2] = val_data[:,2] / 49.0

The training in the previous experiments was possible probably due to the batch normalization layers before the tanh nonlinearities.

Very unstable the loss in the validation set. In comparison, validation loss was always larger that on experiment 1.

Experiment 5:

-> MSE loss
-> Scaling output between 0 and 1
-> Scaling lat and long between 0 and 1


parameters = {
    "batch_size": 2048,
    "normalize_timescale": 480,
    "learning_rate": 0.05,#0.1,
    "weight_decay": 0.0, #1e-4, #1e-4,
    "lambda_l1": None, # l1 loss
    "momentum": 0.9,
    "random_noise": False,
    "noise_scale": None,
    "hidden_size": 96, #128,
    "dropout": 0.0, #0.5,
    "num_epochs": 2500,
    "device": "cpu",
    "last_model": "saved_models/model_5.pt",
    "best_model": "saved_models/best_model_5.pt",
    "plot": "plots/training_5.png",
    "save_every": 1,
    "log_every": 1,
    "n_heads": None
}

[INFO] epoch: 20/2500, training loss: 0.011256969618526375, validation loss: 0.012047839723527434
[INFO] lr: 0.05
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 0.012047839723527434

Experiment 6:

Everything the same as experiment 5,
but changing the number of neighbors: between 20 and 50.
(before it was between 5 and 30)

[INFO] epoch: 39/2500, training loss: 0.008498286558526821, validation loss: 0.009087809757329524
[INFO] lr: 0.05
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 0.009087809757329524

Reducing learning rate and training a little bit more did not gave better results.

Após isso começou a apresentar overfitting.

***** Result in the test set:

Mean squared error: 8588.19119665815
RMSE: 92.67249428313748

Experiment 7:

Everything the same as experiment 5 (including number of neighbors between 5 and 30)

But larger model: hidden state dim 256 instead of 96

parameters = {
    "batch_size": 2048,
    "normalize_timescale": 480,
    "learning_rate": 0.05,#0.1,
    "weight_decay": 0.0, #1e-4, #1e-4,
    "lambda_l1": None, # l1 loss
    "momentum": 0.9,
    "random_noise": False,
    "noise_scale": None,
    "hidden_size": 256, #128,
    "dropout": 0.0, #0.5,
    "num_epochs": 2500,
    "device": "cpu",
    "last_model": "saved_models/model_7.pt",
    "best_model": "saved_models/best_model_7.pt",
    "plot": "plots/training_7.png",
    "save_every": 1,
    "log_every": 1,
    "n_heads": None
}

[INFO] epoch: 33/2500, training loss: 0.011917852786030955, validation loss: 0.011707086861133576
[INFO] lr: 0.05
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 0.011707086861133576

Experiment 8

Same thing as experiment 6

Using MSE for loss

But NOT scaling the output of the NN

Very hard to train, even with learning rate very low validation loss starts increasing on second epoch.

Experiment 10

parameters = {
    "batch_size": 2048,
    "normalize_timescale": 480,
    "learning_rate": 0.005,#0.1,
    "weight_decay": 0.0, #1e-4, #1e-4,
    "lambda_l1": None, # l1 loss
    "momentum": 0.9,
    "random_noise": False,
    "noise_scale": None,
    "hidden_size": 96, #128,
    "dropout": 0.0, #0.5,
    "num_epochs": 2500,
    "device": "cpu",
    "last_model": "saved_models/model_10.pt",
    "best_model": "saved_models/best_model_10.pt",
    "plot": "plots/training_10.png",
    "save_every": 1,
    "log_every": 1,
    "n_heads": None
}

train_dataset = PointNeighborhood(train_data, 
                                  train=True, 
                                  normalize_time_difference=parameters["normalize_timescale"], # training the model to predict looking back at this interval
                                  hidden=parameters["hidden_size"],
                                  random_noise=parameters["random_noise"], 
                                  noise_scale=parameters["noise_scale"],
                                  output_scaler=output_scaler,
                                  min_neighbors=20,
                                  max_neighbors=50)

loss_func = nn.HuberLoss()

[INFO] epoch: 16/2500, training loss: 59.95630905844948, validation loss: 68.8248191833496
[INFO] lr: 0.005
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 68.8248191833496

After this started increasing the validation loss.

Using Huber loss, much more well behaved than using MSE loss. Training went reasonably well until start overfitting.

Did not used any dropout or weight decay. Training loss continued to decrease.

Example 11:

parameters = {
    "batch_size": 2048,
    "normalize_timescale": 480,
    "learning_rate": 0.05,#0.1,
    "weight_decay": 0.0,
    "momentum": 0.2,
    "random_noise": False,
    "noise_scale": None,
    "hidden_size": 128, #96,
    "dropout": 0.5,
    "num_epochs": 2500,
    "device": "cpu",
    "last_model": "saved_models/model_11.pt",
    "best_model": "saved_models/best_model_11.pt",
    "plot": "plots/training_11.png",
    "save_every": 1,
    "log_every": 1,
    "n_heads": 8
}

model = MultiHeadSpatialRegressor(hidden=parameters["hidden_size"], n_head=parameters["n_heads"], prob=parameters["dropout"])

loss_func = nn.HuberLoss()

Model with 8 attention heads, no scaling in the output, Huber loss: training went well, best validation loss so far.

[INFO] lr: 0.05
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 55.540187263488775

*** On the test set:

Mean squared error: 8054.572598681969
RMSE: 89.7472707032474

Experiment 15

The model is training (model3b), albeit very slowly. Looking at the architecture, it doens't seem
that the residual connections are being done correcty. For instance:

[INFO] epoch: 100/2500, training loss: 8771.304526589132, validation loss: 7890.0787109375
[INFO] lr: 0.001
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 7890.0787109375

...
self.x_main_2 = self.activation13_before_aggr(x_main_1)

x_main_2 = self.dropout13_aggr(self.x_main_2)
x_main_2 = self.linear2_before_aggr(x_main_2)
x_main_2 = x_main_2 + self.x_main_2
x_main_2 = self.bn2_before_aggr(x_main_2)
self.x_main_3 = self.activation2_before_aggr(x_main_2)
...

*** Result on test set:
(best result so far)

Mean squared error: 7908.993484816825
RMSE: 88.93252208734904

In this example, the input to the block uses a shortcut only to pass through the linear layer
which is only a matrix multiplication and no bottleneck to the gradinents. While the true bottleneck, 
which is the tanh nonlinearity which squashes the gradient to a max value of 1.0, has no shorcut connection
at all.

It does seem (maily in the inspection of the gpt or nano gpt architecture) that a more appropriate arrangement
would be:

x_main_2 = self.bn2_before_aggr(x_main_1) # batch norm in the beginning of the block
x_main_2 = self.linear2_before_aggr(x_main_2)
self.x_main_2 = self.activation2_before_aggr(x_main_2)
x_main_2 = self.dropout13_aggr(self.x_main_2)
x_main_2 = x_main_2 + x_main_1 # shorcut linking the input to the block to its output

Another factor that can make it hard for the model to train, is that one of the values in its input has a very
different magnitude (the output of the neighbor being analyzed), because it is not being normalized. This decision was
made because it seemed in another experiment the normalizing the input/outputs was hindering the model's ability to learn
the full range of the output. So the decision was made to remove the normalization altogether. 

One possible tradeoff that can be made is to remove the normalization only of the output, leaving the inputs normalized.
This can make the learning of the network weights easier due to its inputs being less varying and more well behaved.

    def forward(self, u, mask):
        # u: (batch, neighbors, features)
        # example: (2, 3, 4)

        # mask: (batch, neighbors, features)
        # example: (2, 3, 4)

        # ------- pass the input features from the set of neighbots in the MLP-theta,
        # responsible for calculating the attention weights

        batch_size = u.shape[0]

        # (batch * neighbors, features)
        input = u.view(-1, self.features)
        
        # (batch * neighbors, hidden)

        x_main = self.linear1_before_aggr(input)
        x_main = self.bn1_before_aggr(x_main)
        self.x_main = self.activation1_before_aggr(x_main)

        x_main_1 = self.dropout1_aggr(self.x_main)
        x_main_1 = self.linear12_before_aggr(x_main_1)
        x_main_1 = x_main_1 + self.x_main
        x_main_1 = self.bn12_before_aggr(x_main_1)
        self.x_main_1 = self.activation12_before_aggr(x_main_1)

        x_main_1 = self.dropout12_aggr(self.x_main_1)
        x_main_1 = self.linear13_before_aggr(x_main_1)
        x_main_1 = x_main_1 + self.x_main_1
        x_main_1 = self.bn13_before_aggr(x_main_1)
        self.x_main_2 = self.activation13_before_aggr(x_main_1)

        x_main_2 = self.dropout13_aggr(self.x_main_2)
        x_main_2 = self.linear2_before_aggr(x_main_2)
        x_main_2 = x_main_2 + self.x_main_2
        x_main_2 = self.bn2_before_aggr(x_main_2)
        self.x_main_3 = self.activation2_before_aggr(x_main_2)

        # (batch, neighbors, hidden)
        x_main_out = self.x_main_3.view(batch_size, -1, self.hidden)
   
        # attention leg

        # (batch * neighbors, features)
        x_attn = self.linear1_attn(input)
        x_attn = self.bn1_attn(x_attn)
        self.x_attn = self.activation1_attn(x_attn) #nn.ReLU()

        x_attn_1 = self.dropout1_attn(self.x_attn)
        x_attn_1 = self.linear12_attn(x_attn_1)
        x_attn_1 = x_attn_1 + self.x_attn
        x_attn_1 = self.bn12_attn(x_attn_1)
        self.x_attn_1 = self.activation12_attn(x_attn_1) #nn.ReLU()

        x_attn_2 = self.dropout12_attn(self.x_attn_1)
        x_attn_2 = self.linear13_attn(x_attn_2)
        x_attn_2 = x_attn_2 + self.x_attn_1
        x_attn_2 = self.bn13_attn(x_attn_2)
        self.x_attn_2 = self.activation13_attn(x_attn_2) #nn.ReLU()

        # (batch * neighbors, hidden)
        x_attn_3 = self.dropout13_attn(self.x_attn_2)
        x_attn_3 = self.linear2_attn(x_attn_3)

        # (batch, neighbors, hidden)
        x_attn_3 = x_attn_3.view(batch_size, -1, self.hidden)

        # (batch, neighbors, hidden)
        x_attn_3 = x_attn_3.masked_fill(mask==0, -float("inf"))

        # (batch, neighbors, features)
        x_attn_3 = self.softmax(x_attn_3) # softmax in the neighbors dim

        x = x_main_out * x_attn_3 # element wise multiplication by the weights

        # (batch, features)
        x = x.sum(dim=1)

        # ------- pass the aggregated node vetor in the MLP-phi, which is responsible for the regression
        #x = self.dropout1(x)

        # (batch, features)
        x0 = self.dropout1_after_aggr(x)
        x0 = self.linear1_after_aggr(x0)
        x0 = x0 + x
        x0 = self.bn1_after_aggr(x0)
        self.x0 = self.activation1_after_aggr(x0)
        
        x_1 = self.dropout12_after_aggr(self.x0)
        x_1 = self.linear12_after_aggr(x_1)
        x_1 = x_1 + self.x0
        x_1 = self.bn12_after_aggr(x_1)
        self.x_1 = self.activation12_after_aggr(x_1)

        x_1 = self.dropout13_after_aggr(self.x_1)
        x_1 = self.linear13_after_aggr(x_1)
        x_1 = x_1 + self.x_1
        x_1 = self.bn13_after_aggr(x_1)
        self.x_2 = self.activation13_after_aggr(x_1)

        x_2 = self.dropout2_after_aggr(self.x_2)
        x_2 = self.linear2_after_aggr(x_2)
        x_2 = x_2 + self.x_2
        x_2 = self.bn2_after_aggr(x_2)
        self.x_3 = self.activation2_after_aggr(x_2)

        x_out = self.dropout3_after_aggr(self.x_3)
        x_out = self.linear3_regression(x_out)

        return x_out

Experiment 16:

  Tried to correct for the shortcomings of experiment 15:

  For instance, the MLP (model3c) block now is:

        x_main_2 = self.bn2_before_aggr(x_main_1)
        x_main_2 = self.linear2_before_aggr(x_main_2)
        self.x_main_2 = self.activation2_before_aggr(x_main_2)
        x_main_2 = self.dropout2_before_aggr(self.x_main_2)
        x_main_2 = x_main_2 + x_main_1

  And we are normalizing the input of the network, but not its output.

Mean squared error: 7738.525494687602
RMSE: 87.96888935690619
