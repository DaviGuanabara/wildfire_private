Experiment 1:

Repeating the best experiment from the fixed set: 
everything the same, learning rate decay, dropout, weight decay, skip connections

[INFO] epoch: 499/2500, training loss: 0.02065398171544075, validation loss: 0.03452903777360916
[INFO] epoch: 599/2500, training loss: 0.02114817686378956, validation loss: 0.034172847867012024
[INFO] epoch: 699/2500, training loss: 0.0181023720651865, validation loss: 0.0422956645488739
[INFO] epoch: 799/2500, training loss: 0.016021419316530228, validation loss: 0.07308559119701385

[INFO] saving best model checkpoint: 0.028773462399840355

Model presented its best values between epochs 500 and 600. After 600 hundred, it presented strong overfitting.
Since epoch 300 the curves presented a larger gap between them than what was observed in the fixed sensors dataset.

Experiment 2:

Leaving learning rate fixed at 0.1

at changing the BN position to directly before the activations:
(this was motivated because the activations were too much saturated, around 70%. With this change it went to aroun 5%.

The validation curve got more spiky but it learned better and with less overfitting. The 
validation loss is smaller than in the best model of the fixed sensors dataset

[INFO] epoch: 299/2500, training loss: 0.02289661392569542, validation loss: 0.045771922916173935
[INFO] epoch: 399/2500, training loss: 0.02105109952390194, validation loss: 0.05100138485431671
[INFO] epoch: 499/2500, training loss: 0.020164381712675095, validation loss: 0.03597750514745712
[INFO] epoch: 599/2500, training loss: 0.021753942593932152, validation loss: 0.029350008815526962
[INFO] epoch: 699/2500, training loss: 0.019479813054203987, validation loss: 0.03051845356822014
[INFO] epoch: 799/2500, training loss: 0.01970771700143814, validation loss: 0.02023000456392765
[INFO] epoch: 899/2500, training loss: 0.01964600570499897, validation loss: 0.024886833503842354

[INFO] epoch: 1099/2500, training loss: 0.020578570663928986, validation loss: 0.021544063463807106
[INFO] lr: 0.1
[INFO] saving model checkpoint
[INFO] saving best model checkpoint: 0.015352508053183556

Experiment 3:

Trying same setting as experiment 2, but with a small learning rate decay:


  lr = (1/(1+0.0005*epoch_index)) * 0.1

Training did not get such good results as experiment 2 and model presented more overfitting.

Experiment 4:

Trying a new model, using the same general architecture but with 4 attention heads, which get
concatenated before feeding the final part of the network.

Training is less stable than before with only one attention head, with more peaky loss curves.

[INFO] epoch: 199/2500, training loss: 0.033938102424144745, validation loss: 0.04652448371052742
[INFO] epoch: 299/2500, training loss: 0.0253498125821352, validation loss: 0.036305982619524
[INFO] epoch: 399/2500, training loss: 0.023393137380480766, validation loss: 0.0479254387319088
[INFO] epoch: 499/2500, training loss: 0.027026813477277756, validation loss: 0.06194622442126274
[INFO] epoch: 599/2500, training loss: 0.024674922227859497, validation loss: 0.04631241038441658
[INFO] epoch: 699/2500, training loss: 0.02815956249833107, validation loss: 0.035467226058244705
[INFO] epoch: 799/2500, training loss: 0.024846302345395088, validation loss: 0.049996014684438705
[INFO] epoch: 899/2500, training loss: 0.02162506990134716, validation loss: 0.026189351454377174
[INFO] epoch: 999/2500, training loss: 0.022145122289657593, validation loss: 0.06502202153205872
[INFO] epoch: 1099/2500, training loss: 0.02214367501437664, validation loss: 0.04206661880016327
[INFO] epoch: 1199/2500, training loss: 0.023352909833192825, validation loss: 0.04301023483276367

Experiment 5:

Analyzing the activations, it appeared that the activations in the linear layer were acting only as pass
through, with no saturation. To improve this, we tried to increase the learning rate to 0.5. This however
seemed to be too much too soon.

We then started trying to warmup the learning rate until 0.5. It worked well until when close to the final
learning rate of 0.5 the validation loss would skyrocket.

It got impressive results in this configuration, but after epoch 500 learning rate got too high and the 
model started oscilating a lot

When it got to 700 hundred, the end of the warmup, the loss skyrocketed.
It seems understandable once the loss increased 10 fold between epochs 599 and 699

Warming up for 700 epochs:

    if epoch_index < 700: # warmup
        lr = (1/(1+0.1*(700-epoch_index))) * 0.5
        optimizer.param_groups[0]['lr'] = lr

[INFO] epoch: 199/2500, training loss: 0.04093082621693611, validation loss: 0.031565744429826736
[INFO] lr: 0.009784735812133072
[INFO] epoch: 299/2500, training loss: 0.030078336596488953, validation loss: 0.026296043768525124
[INFO] lr: 0.0121654501216545
[INFO] epoch: 399/2500, training loss: 0.02527228556573391, validation loss: 0.017908740788698196
[INFO] lr: 0.01607717041800643
[INFO] epoch: 499/2500, training loss: 0.025552259758114815, validation loss: 0.021534033119678497
[INFO] lr: 0.023696682464454975
[INFO] epoch: 599/2500, training loss: 0.02785063162446022, validation loss: 0.0313604399561882
[INFO] lr: 0.045045045045045036
[INFO] epoch: 699/2500, training loss: 56.785919189453125, validation loss: 3.394254207611084
[INFO] lr: 0.45454545454545453

[INFO] saving best model checkpoint: 0.013000017032027245

Experiment 6:

Once the performance in the Experiment 5 is very promising, we try to build on it.

The oscillations during training started after epoch 500. So we continue with the same warmup schedule
until epoch 550 (when lr is smalled than 0.045), and before the lr and loss explode, we start decreasing the learning rate.

[INFO] epoch: 299/2500, training loss: 0.034646280109882355, validation loss: 0.03599095717072487
[INFO] lr: 0.0121654501216545
[INFO] epoch: 399/2500, training loss: 0.026411868631839752, validation loss: 0.02890259027481079
[INFO] lr: 0.01607717041800643
[INFO] epoch: 499/2500, training loss: 0.024740049615502357, validation loss: 0.018405167385935783
[INFO] lr: 0.023696682464454975
[INFO] epoch: 599/2500, training loss: 0.0197006668895483, validation loss: 0.017095524817705154
[INFO] lr: 0.0052637119696810186
[INFO] epoch: 699/2500, training loss: 0.0177312009036541, validation loss: 0.015371938236057758
[INFO] lr: 0.001953201296925661
[INFO] epoch: 799/2500, training loss: 0.018454262986779213, validation loss: 0.014219788834452629
[INFO] lr: 0.0011990695220508883
[INFO] epoch: 899/2500, training loss: 0.017626063898205757, validation loss: 0.015125569887459278
[INFO] lr: 0.0008650668696690254
[INFO] epoch: 999/2500, training loss: 0.01769985631108284, validation loss: 0.014169068075716496
[INFO] lr: 0.0006765991420722877
[INFO] epoch: 1299/2500, training loss: 0.016819309443235397, validation loss: 0.01429012045264244
[INFO] lr: 0.00040916865113462464

[INFO] saving best model checkpoint: 0.01300782710313797


TESTING THE BEST MODEL FROM EXPERIMENT 6

prediction number 100, current error: 0.027147878198812137
prediction number 200, current error: 0.02419650359783863
prediction number 300, current error: 0.021594885144838268
prediction number 400, current error: 0.019330201183463777
prediction number 500, current error: 0.017324983601096123
prediction number 600, current error: 0.015574000017837736
prediction number 700, current error: 0.014055157070214156
prediction number 800, current error: 0.012745520906466572
prediction number 900, current error: 0.011621905159347535
prediction number 1000, current error: 0.010657918511443515
prediction number 1100, current error: 0.009841731098737401
prediction number 1200, current error: 0.009146469290800385
prediction number 1300, current error: 0.008559558967464036
prediction number 1400, current error: 0.00806002532463045
prediction number 1500, current error: 0.007644878888284953
prediction number 1600, current error: 0.007301384377949714
prediction number 1700, current error: 0.007027227222900172
prediction number 1800, current error: 0.006817001288085398
prediction number 1900, current error: 0.006670772327225007
prediction number 2000, current error: 0.006597957456808923
prediction number 2100, current error: 0.006597242985501865
prediction number 2200, current error: 0.00666952894424168
prediction number 2300, current error: 0.006823829911308022
prediction number 2400, current error: 0.00707107197650775
prediction number 2500, current error: 0.007408547980266684
Mean squared error: 0.007411912603535621
